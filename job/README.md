# Video Processing Job

This directory contains the Cloud Run Job for video processing.

## Overview

The video processing job is a **lightweight video assembly service** that follows blueprints generated by the backend API. It operates in a blueprint-based architecture where all intelligence (audio analysis, move selection, AI choreography sequencing) is handled by the API/backend, and the job container simply assembles videos according to the blueprint instructions.

### Blueprint-Based Architecture

The system uses a two-stage architecture:

1. **API/Backend (Intelligence Layer)**
   - Analyzes audio features (tempo, beats, energy) using Librosa
   - Performs in-memory vector search with FAISS to find matching dance moves
   - Generates choreography sequences using Gemini AI
   - Creates complete blueprint JSON with all video assembly instructions
   - Stores blueprint in database for audit trail

2. **Job Container (Execution Layer)**
   - Receives blueprint JSON as environment variable
   - Validates blueprint schema and security
   - Fetches audio and video files from storage (local or GCS)
   - Assembles video using FFmpeg based on blueprint
   - Uploads result to storage
   - Updates task status in database

**Key Benefits:**
- **Lightweight:** Job container has minimal dependencies (FFmpeg, psycopg2, GCS client)
- **Fast:** No audio analysis or AI processing in job container
- **Reproducible:** Blueprints can be stored and re-executed
- **Debuggable:** Complete audit trail of what was generated and how
- **Scalable:** Job containers can scale independently from API

### What the Job Does

1. Parse and validate blueprint JSON from `BLUEPRINT_JSON` environment variable
2. Fetch song audio file from storage (local filesystem or Google Cloud Storage)
3. Fetch all video clip files from storage (parallel downloads for speed)
4. Assemble video with FFmpeg:
   - Concatenate video clips in sequence
   - Add audio track from song
   - Apply transitions (cut, crossfade, fade_black, fade_white)
   - Encode with specified codecs and quality settings
5. Upload result video to storage
6. Update database with completion status and result metadata

### What the Job Does NOT Do

- ❌ Audio analysis (tempo, beats, energy) - handled by API
- ❌ Move searching and selection - handled by API with FAISS
- ❌ AI choreography generation - handled by API with Gemini
- ❌ Elasticsearch queries - removed entirely
- ❌ Machine learning inference - no ML libraries needed

**Note:** This architecture change removes heavy dependencies (Django, Elasticsearch, Librosa, NumPy, SciPy, ML libraries) from the job container, reducing build time from 5+ minutes to under 2 minutes and memory usage from 2GB+ to under 512MB.

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────┐
│                       API/Backend (Intelligence)                 │
│                                                                   │
│  1. Analyze audio (Librosa)                                      │
│  2. Vector search (FAISS)                                        │
│  3. AI sequencing (Gemini)                                       │
│  4. Generate blueprint JSON                                      │
│  5. Store in database                                            │
│  6. Submit job with blueprint                                    │
│                                                                   │
└─────────────────────────────────────────────────────────────────┘
                              │
                              │ BLUEPRINT_JSON (env var)
                              ▼
┌─────────────────────────────────────────────────────────────────┐
│                    Job Container (Execution)                     │
│                                                                   │
│  1. Parse & validate blueprint                                   │
│  2. Fetch audio file (local/GCS)                                 │
│  3. Fetch video clips (parallel)                                 │
│  4. Assemble with FFmpeg                                         │
│  5. Upload result (local/GCS)                                    │
│  6. Update database status                                       │
│                                                                   │
└─────────────────────────────────────────────────────────────────┘
```

## Local Development

### Running with Docker Compose

The job can be run locally using Docker Compose. The API will automatically pass the blueprint JSON when creating job executions.

```bash
# Start all services (API will generate blueprints)
docker-compose --profile microservices up -d

# The API automatically triggers jobs when users request choreography
# Jobs run in the background with blueprints passed as environment variables
```

### Manual Testing with Custom Blueprint

For testing, you can manually run the job with a custom blueprint:

```bash
# Create a test blueprint
export BLUEPRINT_JSON='{
  "task_id": "test-123",
  "audio_path": "data/songs/test.mp3",
  "moves": [
    {
      "clip_id": "move_1",
      "video_path": "data/Bachata_steps/basic_steps/basic_1.mp4",
      "start_time": 0.0,
      "duration": 8.0,
      "transition_type": "cut"
    },
    {
      "clip_id": "move_2",
      "video_path": "data/Bachata_steps/body_roll/body_roll_1.mp4",
      "start_time": 8.0,
      "duration": 8.0,
      "transition_type": "crossfade"
    }
  ],
  "total_duration": 16.0,
  "output_config": {
    "output_path": "data/output/test_choreography.mp4"
  }
}'

# Run the job
docker-compose run --rm \
  -e TASK_ID=test-123 \
  -e USER_ID=1 \
  -e BLUEPRINT_JSON="$BLUEPRINT_JSON" \
  job
```

### Local vs GCS Storage Modes

The job automatically detects the environment and uses the appropriate storage backend:

**Local Mode (Development):**
- Reads files from `data/` directory
- Writes output to `data/output/`
- No GCS credentials needed
- Triggered when `GCS_BUCKET_NAME` is empty

**GCS Mode (Production):**
- Reads files from Google Cloud Storage bucket
- Writes output to GCS bucket
- Requires GCS credentials
- Triggered when `GCS_BUCKET_NAME` is set

```bash
# Local mode (default in docker-compose)
GCS_BUCKET_NAME=

# GCS mode (production)
GCS_BUCKET_NAME=your-bucket-name
```

## Environment Variables

The job container uses environment variables for configuration. The API automatically passes these when creating job executions.

### Required Variables

#### Task Information
| Variable | Description | Example |
|----------|-------------|---------|
| `TASK_ID` | Unique task identifier (UUID string) | `abc-123-def-456` |
| `USER_ID` | User ID who requested the choreography | `1` |

#### Blueprint Configuration
| Variable | Description |
|----------|-------------|
| `BLUEPRINT_JSON` | Complete blueprint JSON with all choreography instructions |

The blueprint is automatically generated by the backend API and contains:
- **task_id**: Links to database task record
- **audio_path**: Path to song audio file
- **audio_tempo**: Song tempo in BPM (optional)
- **moves**: Array of video clips with timing and transitions
- **total_duration**: Expected video duration
- **difficulty_level**: Choreography difficulty (optional)
- **generation_parameters**: Metadata about generation (optional)
- **output_config**: Video encoding settings

**Example Blueprint Structure:**
```json
{
  "task_id": "abc-123",
  "audio_path": "data/songs/Amor.mp3",
  "audio_tempo": 129.2,
  "moves": [
    {
      "clip_id": "move_1",
      "video_path": "data/Bachata_steps/body_roll/body_roll_1.mp4",
      "start_time": 0.0,
      "duration": 8.0,
      "transition_type": "cut"
    },
    {
      "clip_id": "move_2",
      "video_path": "data/Bachata_steps/body_roll/body_roll_3.mp4",
      "start_time": 8.0,
      "duration": 8.0,
      "transition_type": "crossfade"
    }
  ],
  "total_duration": 16.0,
  "difficulty_level": "intermediate",
  "output_config": {
    "output_path": "data/output/choreography_abc-123.mp4",
    "output_format": "mp4",
    "video_codec": "libx264",
    "audio_codec": "aac"
  }
}
```

See [Blueprint Schema Documentation](../docs/BLUEPRINT_SCHEMA.md) for complete schema details.

#### Database Configuration
| Variable | Description | Local Example | Production Example |
|----------|-------------|---------------|-------------------|
| `DB_HOST` | Database host | `db` | `/cloudsql/PROJECT:REGION:INSTANCE` |
| `DB_PORT` | Database port | `5432` | `5432` |
| `DB_NAME` | Database name | `bachata_vibes` | `bachata_buddy` |
| `DB_USER` | Database user | `postgres` | `postgres` |
| `DB_PASSWORD` | Database password | `postgres` | `your-secure-password` |

**Note:** In production (Cloud Run), the job automatically detects the Cloud SQL Unix socket connection when `CLOUD_SQL_CONNECTION_NAME` is set.

#### Google Cloud Configuration
| Variable | Description | Example |
|----------|-------------|---------|
| `GCP_PROJECT_ID` | Google Cloud project ID | `your-project-id` |
| `GCP_REGION` | Google Cloud region | `us-central1` |

#### Cloud Storage Configuration
| Variable | Description | Local | Production |
|----------|-------------|-------|------------|
| `GCS_BUCKET_NAME` | GCS bucket for videos/audio | *(empty)* | `your-bucket-name` |

**Storage Mode Detection:**
- **Empty or not set**: Uses local filesystem (`data/` directory)
- **Set to bucket name**: Uses Google Cloud Storage

### Optional Variables

#### FFmpeg Configuration
| Variable | Default | Description |
|----------|---------|-------------|
| `VIDEO_OUTPUT_FORMAT` | `mp4` | Video container format |
| `VIDEO_CODEC` | `libx264` | Video codec (h264) |
| `VIDEO_CRF` | `23` | Video quality (0-51, lower = better) |
| `VIDEO_FPS` | `30` | Video frame rate |
| `AUDIO_CODEC` | `aac` | Audio codec |
| `AUDIO_BITRATE` | `128k` | Audio bitrate |

**FFmpeg Quality Settings:**
- **CRF 18-23**: High quality (recommended for final output)
- **CRF 23-28**: Medium quality (good balance)
- **CRF 28-35**: Lower quality (smaller file size)

#### Logging Configuration
| Variable | Default | Description |
|----------|---------|-------------|
| `LOG_LEVEL` | `INFO` | Logging verbosity |

**Log Levels:**
- `DEBUG`: Detailed information for debugging (development only)
- `INFO`: Normal operations and progress updates (default)
- `WARNING`: Recoverable errors or unexpected situations
- `ERROR`: Failed operations that don't stop the job
- `CRITICAL`: System failures that stop the job

**Structured Logging:**
All log messages include structured data (task_id, user_id, timestamps) for easy filtering and analysis in production.

#### Python Configuration
| Variable | Default | Description |
|----------|---------|-------------|
| `PYTHONDONTWRITEBYTECODE` | `1` | Disable bytecode generation |
| `PYTHONUNBUFFERED` | `1` | Force unbuffered output for real-time logging |

### Environment Variable Files

- **`.env.example`**: Complete template with all variables and descriptions
- **`docker-compose.yml`**: Default values for local development
- **Cloud Run Job**: Variables passed by API when creating executions

### Variables Removed in Blueprint Architecture

The following variables are **no longer used** (intelligence moved to API):

- ❌ `ELASTICSEARCH_HOST`, `ELASTICSEARCH_PORT`, `ELASTICSEARCH_API_KEY`, `ELASTICSEARCH_INDEX` - Elasticsearch removed
- ❌ `AUDIO_INPUT`, `DIFFICULTY`, `ENERGY_LEVEL`, `STYLE` - Now in blueprint
- ❌ `GOOGLE_API_KEY` - AI processing in API, not job
- ❌ `YOLOV8_MODEL`, `YOLOV8_CONFIDENCE`, `YOLOV8_DEVICE` - No pose detection in job

All choreography parameters are now passed via the `BLUEPRINT_JSON` variable.

## Deployment

### Local Development (Docker Compose)

The job runs automatically when the API creates choreography tasks:

```bash
# Start all services
docker-compose --profile microservices up -d

# Jobs run in the background when triggered by API
# View job logs
docker-compose logs -f job
```

### Production (Cloud Run Jobs)

#### Initial Setup

1. **Build and push Docker image:**
   ```bash
   # Build image
   docker build -t gcr.io/PROJECT_ID/video-processor:latest -f job/Dockerfile .
   
   # Push to Google Container Registry
   docker push gcr.io/PROJECT_ID/video-processor:latest
   ```

2. **Create Cloud Run Job:**
   ```bash
   gcloud run jobs create video-processor \
     --image=gcr.io/PROJECT_ID/video-processor:latest \
     --region=us-central1 \
     --memory=512Mi \
     --cpu=1 \
     --timeout=5m \
     --max-retries=3 \
     --set-env-vars="DB_NAME=bachata_buddy,GCP_PROJECT_ID=PROJECT_ID,GCP_REGION=us-central1,GCS_BUCKET_NAME=your-bucket" \
     --set-secrets="DB_PASSWORD=db-password:latest" \
     --set-cloudsql-instances=PROJECT_ID:REGION:INSTANCE_NAME \
     --service-account=video-processor@PROJECT_ID.iam.gserviceaccount.com
   ```

3. **Grant permissions:**
   ```bash
   # Allow API to execute jobs
   gcloud run jobs add-iam-policy-binding video-processor \
     --region=us-central1 \
     --member="serviceAccount:api@PROJECT_ID.iam.gserviceaccount.com" \
     --role="roles/run.invoker"
   
   # Allow job to access Cloud SQL
   gcloud projects add-iam-policy-binding PROJECT_ID \
     --member="serviceAccount:video-processor@PROJECT_ID.iam.gserviceaccount.com" \
     --role="roles/cloudsql.client"
   
   # Allow job to access Cloud Storage
   gcloud projects add-iam-policy-binding PROJECT_ID \
     --member="serviceAccount:video-processor@PROJECT_ID.iam.gserviceaccount.com" \
     --role="roles/storage.objectAdmin"
   ```

#### Updating the Job

```bash
# Build new image
docker build -t gcr.io/PROJECT_ID/video-processor:latest -f job/Dockerfile .
docker push gcr.io/PROJECT_ID/video-processor:latest

# Update job
gcloud run jobs update video-processor \
  --image=gcr.io/PROJECT_ID/video-processor:latest \
  --region=us-central1
```

#### Resource Configuration

**Recommended Settings:**
- **Memory:** 512Mi (sufficient for video assembly)
- **CPU:** 1 (single-threaded FFmpeg)
- **Timeout:** 5 minutes (3-minute video assembles in <30s)
- **Max Retries:** 3 (retry on transient failures)

**For larger videos or more clips:**
```bash
gcloud run jobs update video-processor \
  --memory=1Gi \
  --timeout=10m
```

#### Environment Variables

The API automatically passes these when creating executions:
- `TASK_ID` - Task identifier
- `USER_ID` - User identifier
- `BLUEPRINT_JSON` - Complete blueprint

Static environment variables (set in job configuration):
- `DB_NAME` - Database name
- `GCP_PROJECT_ID` - Project ID
- `GCP_REGION` - Region
- `GCS_BUCKET_NAME` - Storage bucket
- `LOG_LEVEL` - Logging level (INFO)

Secrets (from Secret Manager):
- `DB_PASSWORD` - Database password

#### Monitoring

**View job executions:**
```bash
gcloud run jobs executions list --job=video-processor --region=us-central1
```

**View execution logs:**
```bash
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=video-processor" --limit 50
```

**Monitor metrics:**
- Execution count
- Success rate
- Average duration
- Memory usage
- Error rate

**Set up alerts:**
```bash
# Alert on high failure rate
gcloud alpha monitoring policies create \
  --notification-channels=CHANNEL_ID \
  --display-name="Video Processor Failures" \
  --condition-display-name="Failure rate > 5%" \
  --condition-threshold-value=0.05 \
  --condition-threshold-duration=300s
```

## Logging

The job uses Python's standard logging module with structured logging support.

### Log Levels
- **DEBUG**: Detailed information for debugging (development only)
- **INFO**: Normal operations and progress updates (default)
- **WARNING**: Recoverable errors or unexpected situations
- **ERROR**: Failed operations that don't stop the job
- **CRITICAL**: System failures that stop the job

### Structured Logging

All log messages include structured data for easy filtering and analysis:

```python
logger.info(
    "Blueprint parsed successfully",
    extra={
        'task_id': task_id,
        'user_id': user_id,
        'num_moves': len(blueprint['moves']),
        'total_duration': blueprint['total_duration'],
        'timestamp': datetime.utcnow().isoformat()
    }
)
```

### Log Format

**Local Development:**
```
YYYY-MM-DD HH:MM:SS - module_name - LEVEL - message
```

**Production (Cloud Run):**
```json
{
  "severity": "INFO",
  "message": "Blueprint parsed successfully",
  "task_id": "abc-123",
  "user_id": 1,
  "num_moves": 29,
  "total_duration": 232.0,
  "timestamp": "2025-11-09T12:34:56Z"
}
```

### Example Log Output

```
2025-11-09 12:34:56 - main - INFO - Job started
2025-11-09 12:34:56 - blueprint_parser - INFO - Blueprint parsed successfully
2025-11-09 12:34:56 - blueprint_parser - INFO - Validation passed
2025-11-09 12:34:57 - storage_service - INFO - Downloading audio file: songs/Amor.mp3
2025-11-09 12:34:58 - storage_service - INFO - Downloading 29 video clips in parallel
2025-11-09 12:35:05 - video_assembler - INFO - Assembling video with FFmpeg
2025-11-09 12:35:28 - video_assembler - INFO - Video assembly complete: 227.18s duration
2025-11-09 12:35:29 - storage_service - INFO - Uploading result: output/choreography_abc-123.mp4
2025-11-09 12:35:32 - database - INFO - Task status updated: completed
2025-11-09 12:35:32 - main - INFO - Job completed successfully in 36.2s
```

### Viewing Logs

**Local Development:**
```bash
# View job logs in real-time
docker-compose logs -f job

# View last 100 lines
docker-compose logs --tail=100 job

# Filter by log level
docker-compose logs job | grep ERROR
```

**Production (Cloud Run Jobs):**
```bash
# View recent logs
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=video-processor" --limit 50

# Stream logs in real-time
gcloud logging tail "resource.type=cloud_run_job AND resource.labels.job_name=video-processor"

# Filter by task ID
gcloud logging read "resource.type=cloud_run_job AND jsonPayload.task_id=abc-123" --limit 50

# Filter by severity
gcloud logging read "resource.type=cloud_run_job AND severity>=ERROR" --limit 50
```

### Log Retention

- **Local:** Logs stored in Docker until container is removed
- **Production:** Cloud Logging retains logs for 30 days by default
- **Long-term storage:** Export logs to Cloud Storage or BigQuery for analysis

## Blueprint-Based Video Assembly

### How It Works

The job container follows a simple, linear workflow:

```
1. Parse Blueprint
   ↓
2. Validate Schema & Security
   ↓
3. Fetch Media Files (parallel)
   ↓
4. Assemble Video (FFmpeg)
   ↓
5. Upload Result
   ↓
6. Update Database
```

### Step-by-Step Process

#### 1. Parse Blueprint
```python
# Read from environment variable
blueprint_json = os.getenv('BLUEPRINT_JSON')
blueprint = json.loads(blueprint_json)
```

#### 2. Validate Schema & Security
- Check required fields (task_id, audio_path, moves, output_config)
- Validate data types and value constraints
- Security validation: no directory traversal (`..`), no absolute paths
- Validate transition types and other enums

#### 3. Fetch Media Files
**Local Mode:**
```python
# Read from local filesystem
audio_file = open(f"data/songs/Amor.mp3", 'rb')
video_clips = [open(f"data/Bachata_steps/body_roll/body_roll_1.mp4", 'rb'), ...]
```

**GCS Mode:**
```python
# Download from Google Cloud Storage (parallel)
storage_service.download_file("songs/Amor.mp3", "/tmp/Amor.mp3")
storage_service.download_files(video_paths, "/tmp/clips/")  # Parallel
```

#### 4. Assemble Video with FFmpeg

**Step 4a: Create concat file**
```bash
# concat.txt
file '/tmp/clips/body_roll_1.mp4'
file '/tmp/clips/body_roll_3.mp4'
file '/tmp/clips/combination_3.mp4'
```

**Step 4b: Concatenate video clips**
```bash
ffmpeg -f concat -safe 0 -i concat.txt -c copy /tmp/temp_video.mp4
```

**Step 4c: Add audio track**
```bash
ffmpeg -i /tmp/temp_video.mp4 -i /tmp/Amor.mp3 \
  -c:v copy -c:a aac -b:a 128k \
  -shortest /tmp/output.mp4
```

**Step 4d: Apply transitions (optional)**
```bash
# Crossfade between clips
ffmpeg -i clip1.mp4 -i clip2.mp4 \
  -filter_complex "[0:v][1:v]xfade=transition=fade:duration=0.5:offset=7.5" \
  output.mp4
```

#### 5. Upload Result
**Local Mode:**
```python
# Save to local filesystem
shutil.copy("/tmp/output.mp4", "data/output/choreography_abc-123.mp4")
```

**GCS Mode:**
```python
# Upload to Google Cloud Storage
storage_service.upload_file("/tmp/output.mp4", "output/choreography_abc-123.mp4")
```

#### 6. Update Database
```python
# Update task status
database_service.update_task_status(
    task_id="abc-123",
    status="completed",
    progress=100,
    result={
        "video_url": "/media/output/choreography_abc-123.mp4",
        "duration": 227.18,
        "file_size": 36992903
    }
)
```

### Transition Types

The job supports multiple transition types between video clips:

| Type | Description | FFmpeg Implementation |
|------|-------------|----------------------|
| `cut` | Instant transition (no effect) | Simple concatenation |
| `crossfade` | Gradual blend between clips | `xfade` filter |
| `fade_black` | Fade to black, then fade in | `fade` filter to/from black |
| `fade_white` | Fade to white, then fade in | `fade` filter to/from white |

### Error Handling

The job implements comprehensive error handling with retries:

**Blueprint Validation Errors:**
- Invalid schema → Log error, update DB status to "failed", exit code 1
- Missing required fields → Log error, update DB status to "failed", exit code 1
- Security violations → Log error, update DB status to "failed", exit code 1

**Media File Errors:**
- File not found → Retry 3 times with exponential backoff
- Still failing → Log error, update DB status to "failed", exit code 1

**FFmpeg Errors:**
- Command fails → Log FFmpeg output, update DB status to "failed", exit code 1
- Invalid codec → Log error, update DB status to "failed", exit code 1

**Storage Errors:**
- Upload fails → Retry 3 times with exponential backoff
- Still failing → Log error, update DB status to "failed", exit code 1

**Database Errors:**
- Connection fails → Retry 3 times with exponential backoff
- Still failing → Log error, exit code 1

All errors are logged to stdout with structured logging and recorded in the database for debugging.

## Dependencies

The job container has **minimal dependencies** for fast builds and low memory usage:

### Core Dependencies
- **Python 3.12** - Runtime
- **FFmpeg** - Video assembly and encoding
- **psycopg2-binary** - PostgreSQL database client
- **google-cloud-storage** - Google Cloud Storage client
- **python-dotenv** - Environment variable management

### Removed Dependencies (Now in API)
- ❌ Django, Django REST Framework
- ❌ Elasticsearch client
- ❌ Librosa (audio analysis)
- ❌ NumPy, SciPy (numerical computing)
- ❌ FAISS (vector search)
- ❌ Ultralytics (YOLOv8 pose detection)
- ❌ Google Generative AI (Gemini)

**Build Time:** < 2 minutes (down from 5+ minutes)
**Memory Usage:** < 512MB (down from 2GB+)
**Image Size:** ~500MB (down from 2GB+)


## Testing

The job includes comprehensive test scripts to validate blueprint-based video assembly.

### Blueprint Parser Tests

Test blueprint parsing and validation:

```bash
cd bachata_buddy
uv run python job/test_blueprint_parser.py
```

**What it tests:**
- ✅ Valid blueprint parsing
- ✅ Required field validation
- ✅ Type validation
- ✅ Security validation (path traversal, absolute paths)
- ✅ Transition type validation
- ✅ Value constraint validation
- ✅ Error message formatting

### Video Assembler Tests

Test video assembly logic:

```bash
uv run python job/test_video_assembler.py
```

**What it tests:**
- ✅ FFmpeg command generation
- ✅ Clip concatenation
- ✅ Audio track addition
- ✅ Transition application
- ✅ Output encoding settings
- ✅ Error handling

### Storage Service Tests

Test storage operations (local and GCS):

```bash
uv run python job/test_storage_service.py
```

**What it tests:**
- ✅ Local filesystem operations
- ✅ GCS upload/download
- ✅ Parallel downloads
- ✅ File existence checks
- ✅ Error handling and retries

### Database Service Tests

Test database status updates:

```bash
uv run python job/test_database_service.py
```

**What it tests:**
- ✅ Task status updates
- ✅ Progress tracking
- ✅ Result metadata storage
- ✅ Error message recording
- ✅ Connection pooling
- ✅ Retry logic

### Integration Tests

Test complete blueprint flow:

```bash
# Run from backend directory
cd bachata_buddy/backend
uv run python test_blueprint_flow.py
```

**What it tests:**
- ✅ API blueprint generation
- ✅ Job submission with blueprint
- ✅ Video assembly from blueprint
- ✅ Local storage mode
- ✅ GCS storage mode
- ✅ Error scenarios

### Performance Tests

Test performance requirements:

```bash
cd bachata_buddy/backend
uv run python test_blueprint_performance.py
```

**What it tests:**
- ✅ Blueprint generation < 10 seconds
- ✅ Video assembly < 30 seconds (3-minute video)
- ✅ Memory usage < 512MB
- ✅ Build time < 2 minutes

### Running All Tests

```bash
# Run all job container tests
cd bachata_buddy
uv run python -m pytest job/test_*.py -v

# Run with coverage
uv run python -m pytest job/test_*.py --cov=job/src --cov-report=html
```

### Test Prerequisites

Before running tests, ensure:

1. **Services are running:**
   ```bash
   docker-compose --profile microservices up -d
   ```

2. **Services are healthy:**
   ```bash
   # Check API
   curl http://localhost:8001/health/
   
   # Check database
   docker-compose exec db psql -U postgres -d bachata_vibes -c "SELECT 1;"
   ```

3. **Test data exists:**
   ```bash
   # Audio files
   ls -lh data/songs/
   
   # Video clips
   ls -lh data/Bachata_steps/
   ```

### Legacy Tests (Pre-Blueprint Architecture)

The following tests are from the old architecture and may not work with the blueprint-based system:

- ~~`test_audio_inputs.py`~~ - Audio analysis now in API
- ~~`test_music_analyzer.py`~~ - Music analysis now in API
- ~~`test_elasticsearch_service.py`~~ - Elasticsearch removed
- ~~`test_pose_detector.py`~~ - Pose detection removed
- ~~`test_pipeline.py`~~ - Old pipeline replaced with blueprint assembly

These tests are kept for reference but should not be used for validating the new architecture.

## Usage Examples

### Example 1: Simple 2-Move Choreography

```bash
export BLUEPRINT_JSON='{
  "task_id": "simple-123",
  "audio_path": "data/songs/test.mp3",
  "moves": [
    {
      "clip_id": "move_1",
      "video_path": "data/Bachata_steps/basic_steps/basic_1.mp4",
      "start_time": 0.0,
      "duration": 8.0,
      "transition_type": "cut"
    },
    {
      "clip_id": "move_2",
      "video_path": "data/Bachata_steps/body_roll/body_roll_1.mp4",
      "start_time": 8.0,
      "duration": 8.0,
      "transition_type": "crossfade"
    }
  ],
  "total_duration": 16.0,
  "output_config": {
    "output_path": "data/output/simple_choreography.mp4"
  }
}'

docker-compose run --rm \
  -e TASK_ID=simple-123 \
  -e USER_ID=1 \
  -e BLUEPRINT_JSON="$BLUEPRINT_JSON" \
  job
```

### Example 2: Complex Choreography with Transitions

```bash
export BLUEPRINT_JSON='{
  "task_id": "complex-456",
  "audio_path": "data/songs/Amor.mp3",
  "audio_tempo": 129.2,
  "moves": [
    {
      "clip_id": "move_1",
      "video_path": "data/Bachata_steps/basic_steps/basic_1.mp4",
      "start_time": 0.0,
      "duration": 8.0,
      "transition_type": "cut"
    },
    {
      "clip_id": "move_2",
      "video_path": "data/Bachata_steps/body_roll/body_roll_1.mp4",
      "start_time": 8.0,
      "duration": 8.0,
      "transition_type": "crossfade"
    },
    {
      "clip_id": "move_3",
      "video_path": "data/Bachata_steps/cross_body_lead/cross_body_lead_1.mp4",
      "start_time": 16.0,
      "duration": 8.0,
      "transition_type": "crossfade"
    }
  ],
  "total_duration": 24.0,
  "difficulty_level": "intermediate",
  "output_config": {
    "output_path": "data/output/complex_choreography.mp4",
    "video_codec": "libx264",
    "audio_codec": "aac",
    "video_bitrate": "2M",
    "audio_bitrate": "128k"
  }
}'

docker-compose run --rm \
  -e TASK_ID=complex-456 \
  -e USER_ID=1 \
  -e BLUEPRINT_JSON="$BLUEPRINT_JSON" \
  job
```

### Example 3: GCS Storage Mode

```bash
export BLUEPRINT_JSON='{
  "task_id": "gcs-789",
  "audio_path": "songs/Amor.mp3",
  "moves": [
    {
      "clip_id": "move_1",
      "video_path": "Bachata_steps/basic_steps/basic_1.mp4",
      "start_time": 0.0,
      "duration": 8.0
    }
  ],
  "total_duration": 8.0,
  "output_config": {
    "output_path": "output/gcs_choreography.mp4"
  }
}'

docker-compose run --rm \
  -e TASK_ID=gcs-789 \
  -e USER_ID=1 \
  -e GCS_BUCKET_NAME=your-bucket-name \
  -e BLUEPRINT_JSON="$BLUEPRINT_JSON" \
  job
```

### Example 4: Debug Mode

```bash
export BLUEPRINT_JSON='{ ... }'

docker-compose run --rm \
  -e TASK_ID=debug-001 \
  -e USER_ID=1 \
  -e LOG_LEVEL=DEBUG \
  -e BLUEPRINT_JSON="$BLUEPRINT_JSON" \
  job
```

## Troubleshooting

### Common Issues

#### Blueprint Not Found

**Symptom:** `BLUEPRINT_JSON environment variable not set`

**Solution:**
```bash
# Ensure BLUEPRINT_JSON is set
echo $BLUEPRINT_JSON

# If empty, set it
export BLUEPRINT_JSON='{ ... }'
```

#### Invalid Blueprint Schema

**Symptom:** `Blueprint validation failed: Missing required field: audio_path`

**Solution:**
- Check blueprint against schema in [Blueprint Schema Documentation](../docs/BLUEPRINT_SCHEMA.md)
- Ensure all required fields are present
- Validate JSON syntax with `jq`:
  ```bash
  echo $BLUEPRINT_JSON | jq .
  ```

#### File Not Found

**Symptom:** `Video file not found: data/Bachata_steps/basic_steps/basic_1.mp4`

**Solution:**
```bash
# Check file exists
ls -lh data/Bachata_steps/basic_steps/basic_1.mp4

# Check file path in blueprint matches actual location
# Paths should be relative to /app in container

# For GCS mode, check bucket
gsutil ls gs://your-bucket/Bachata_steps/basic_steps/
```

#### FFmpeg Fails

**Symptom:** `FFmpeg command failed with exit code 1`

**Solution:**
```bash
# Check FFmpeg logs in job output
docker-compose logs job

# Common issues:
# - Invalid video codec
# - Incompatible video formats
# - Corrupted video files
# - Insufficient disk space

# Test FFmpeg manually
docker-compose run --rm job ffmpeg -version
```

#### Database Connection Failed

**Symptom:** `Could not connect to database`

**Solution:**
```bash
# Check database is running
docker-compose ps db

# Check database connection
docker-compose exec db psql -U postgres -d bachata_vibes -c "SELECT 1;"

# Check database credentials in .env
cat .env | grep DB_
```

#### GCS Upload Failed

**Symptom:** `Failed to upload to GCS: 403 Forbidden`

**Solution:**
```bash
# Check GCS credentials
gcloud auth list

# Check bucket permissions
gsutil iam get gs://your-bucket-name

# Check bucket exists
gsutil ls gs://your-bucket-name
```

#### Job Timeout

**Symptom:** Job runs for too long and times out

**Solution:**
```bash
# Check video duration and number of clips
# Large videos (>5 minutes) or many clips (>100) may take longer

# Increase timeout in Cloud Run Job
gcloud run jobs update video-processor --timeout=10m

# For docker-compose, no timeout by default
```

### Debug Mode

Run the job with debug logging to see detailed information:

```bash
docker-compose run --rm \
  -e LOG_LEVEL=DEBUG \
  -e TASK_ID=debug-001 \
  -e USER_ID=1 \
  -e BLUEPRINT_JSON="$BLUEPRINT_JSON" \
  job
```

**Debug output includes:**
- Blueprint parsing details
- File download progress
- FFmpeg commands and output
- Storage operations
- Database queries
- Timing information

### View Logs

**Local Development:**
```bash
# View job logs
docker-compose logs job

# Follow logs in real-time
docker-compose logs -f job

# View last 100 lines
docker-compose logs --tail=100 job
```

**Production (Cloud Run Jobs):**
```bash
# View logs in Cloud Console
gcloud logging read "resource.type=cloud_run_job AND resource.labels.job_name=video-processor" --limit 50

# Stream logs in real-time
gcloud logging tail "resource.type=cloud_run_job AND resource.labels.job_name=video-processor"

# Filter by task ID
gcloud logging read "resource.type=cloud_run_job AND jsonPayload.task_id=abc-123" --limit 50
```

### Performance Issues

**Symptom:** Job takes too long to complete

**Diagnosis:**
```bash
# Check logs for timing information
docker-compose logs job | grep "duration"

# Common bottlenecks:
# - Slow storage downloads (GCS network)
# - Large video files (>100MB per clip)
# - Many clips (>50 clips)
# - Complex transitions (crossfade is slower than cut)
```

**Solutions:**
- Use local storage for development (faster than GCS)
- Reduce video quality (lower bitrate, lower resolution)
- Use simpler transitions (cut instead of crossfade)
- Optimize video clips (compress before uploading)
- Use parallel downloads (already implemented)

### Memory Issues

**Symptom:** Job crashes with out-of-memory error

**Diagnosis:**
```bash
# Check memory usage
docker stats job

# Check video file sizes
ls -lh data/Bachata_steps/**/*.mp4
```

**Solutions:**
- Reduce video quality (lower bitrate)
- Process fewer clips at once
- Increase memory limit in Cloud Run Job:
  ```bash
  gcloud run jobs update video-processor --memory=1Gi
  ```

### Getting Help

If you encounter issues not covered here:

1. **Check logs** with `LOG_LEVEL=DEBUG`
2. **Validate blueprint** against schema
3. **Test with minimal blueprint** (2 clips, short duration)
4. **Check related documentation:**
   - [Blueprint Schema](../docs/BLUEPRINT_SCHEMA.md)
   - [Blueprint Parser README](src/services/README_BLUEPRINT_PARSER.md)
   - [Video Assembler README](src/services/README_VIDEO_ASSEMBLER.md)
5. **Review test files** for examples:
   - `test_blueprint_parser.py`
   - `test_video_assembler.py`
   - `test_storage_service.py`


## Performance

### Benchmarks

Based on testing with typical choreographies:

| Metric | Target | Actual |
|--------|--------|--------|
| Blueprint parsing | < 10ms | ~5ms |
| Video assembly (3-min) | < 30s | ~25s |
| Memory usage | < 512MB | ~350MB |
| Build time | < 2 minutes | ~90s |
| Storage download (29 clips) | < 10s | ~7s (parallel) |
| Storage upload (40MB) | < 5s | ~3s |

### Optimization Tips

**For faster video assembly:**
1. Use `cut` transitions instead of `crossfade` (instant vs. processing)
2. Use lower video quality (CRF 28 vs. 23)
3. Use local storage for development (faster than GCS)
4. Reduce number of clips (fewer concatenations)

**For lower memory usage:**
1. Use lower video bitrate (1M vs. 2M)
2. Process clips in batches (not implemented yet)
3. Clean up temporary files immediately

**For faster builds:**
1. Use Docker layer caching
2. Pin dependency versions
3. Use slim base image (already using python:3.12-slim)

## Security

### Path Validation

All file paths in blueprints are validated for security:

- ❌ No directory traversal: `../../../etc/passwd`
- ❌ No absolute paths: `/etc/passwd`
- ❌ No null bytes: `data/songs\0.mp3`
- ✅ Relative paths only: `data/songs/Amor.mp3`

### Input Sanitization

- Blueprint JSON is parsed and validated before use
- All user inputs are sanitized by the API before blueprint generation
- FFmpeg commands use parameterized arguments (no shell injection)

### Secrets Management

- Database passwords stored in Google Secret Manager
- No secrets in environment variables or logs
- Service account credentials managed by Google Cloud

### Network Security

- Job container only connects to:
  - PostgreSQL database (Cloud SQL)
  - Google Cloud Storage (authenticated)
- No external API calls
- No internet access required (except GCS)

## Migration from Old Architecture

### What Changed

**Before (Old Architecture):**
- Job container had 50+ dependencies
- Job performed audio analysis, Elasticsearch queries, AI generation
- Build time: 5+ minutes
- Memory usage: 2GB+
- Complex error handling across multiple services

**After (Blueprint Architecture):**
- Job container has 5 core dependencies
- Job only assembles videos from blueprints
- Build time: < 2 minutes
- Memory usage: < 512MB
- Simple, linear workflow

### Migration Steps

1. **Update API** to generate blueprints (already done)
2. **Update job container** to use blueprints (already done)
3. **Remove Elasticsearch** from infrastructure (already done)
4. **Update environment variables** (see [Environment Variables Update](../.kiro/specs/blueprint-job-refactor/ENVIRONMENT_VARIABLES_UPDATE.md))
5. **Test with both storage modes** (local and GCS)
6. **Deploy to production** (see Deployment section)

### Rollback Plan

If issues arise, you can temporarily revert:

1. Keep old job container image available
2. Feature flag to switch between architectures
3. Monitor error rates and performance
4. Rollback if error rate > 5% or performance degrades > 50%

## Related Documentation

### Blueprint Documentation
- [Blueprint Schema](../docs/BLUEPRINT_SCHEMA.md) - Complete schema specification
- [Blueprint Parser README](src/services/README_BLUEPRINT_PARSER.md) - Parser implementation
- [Video Assembler README](src/services/README_VIDEO_ASSEMBLER.md) - Video assembly implementation

### API Documentation
- [API Documentation](../backend/API_DOCUMENTATION_UPDATE.md) - API endpoints
- [Blueprint Generator](../backend/services/blueprint_generator.py) - Blueprint generation service
- [Vector Search Service](../backend/services/vector_search_service.py) - FAISS-based search

### Specification Documents
- [Requirements](../.kiro/specs/blueprint-job-refactor/requirements.md) - System requirements
- [Design](../.kiro/specs/blueprint-job-refactor/design.md) - Architecture design
- [Tasks](../.kiro/specs/blueprint-job-refactor/tasks.md) - Implementation tasks
- [Environment Variables Update](../.kiro/specs/blueprint-job-refactor/ENVIRONMENT_VARIABLES_UPDATE.md) - Variable changes

### Testing Documentation
- [Blueprint Flow Tests](../backend/test_blueprint_flow.py) - Integration tests
- [Performance Tests](../backend/test_blueprint_performance.py) - Performance benchmarks

## FAQ

### Why blueprint-based architecture?

**Separation of concerns:** Intelligence (AI, analysis) in API, execution (video assembly) in job container.

**Benefits:**
- Faster builds (< 2 minutes vs. 5+ minutes)
- Lower memory (< 512MB vs. 2GB+)
- Easier debugging (complete audit trail)
- Better scalability (job containers scale independently)
- Reproducible results (same blueprint = same video)

### Can I modify a blueprint after generation?

Yes! Blueprints are stored in the database and can be modified before execution. This allows for:
- Manual adjustments to move selection
- Custom transition types
- Different output settings
- A/B testing different sequences

### How do I add new transition types?

1. Add transition type to blueprint schema validation
2. Implement FFmpeg command in `video_assembler.py`
3. Update documentation
4. Add tests

Example transitions:
- `wipe_left`, `wipe_right` - Directional wipes
- `zoom_in`, `zoom_out` - Zoom effects
- `rotate` - Rotation transition

### Can I use this for non-dance videos?

Yes! The blueprint architecture is generic and can be used for any video assembly task:
- Educational videos (lecture clips)
- Marketing videos (product demos)
- Training videos (tutorial segments)
- Highlight reels (sports clips)

Just generate blueprints with your video clips and the job will assemble them.

### What video formats are supported?

**Input formats:** Any format supported by FFmpeg (mp4, mov, avi, mkv, webm, etc.)

**Output formats:** Configurable in blueprint (default: mp4 with h264/aac)

**Recommended:** Use mp4 with h264 video and aac audio for best compatibility.

### How do I optimize for mobile devices?

Use lower quality settings in blueprint:

```json
{
  "output_config": {
    "video_codec": "libx264",
    "video_bitrate": "1M",
    "audio_bitrate": "96k",
    "frame_rate": 24
  }
}
```

### Can I run multiple jobs in parallel?

**Local (Docker Compose):** Limited by system resources

**Production (Cloud Run Jobs):** Yes! Cloud Run automatically scales:
- Default: 100 concurrent executions
- Can be increased to 1000+ with quota increase
- Each execution is isolated

### How do I handle very long videos (>10 minutes)?

1. Increase timeout:
   ```bash
   gcloud run jobs update video-processor --timeout=15m
   ```

2. Increase memory if needed:
   ```bash
   gcloud run jobs update video-processor --memory=1Gi
   ```

3. Consider splitting into multiple shorter videos

### What happens if a job fails?

1. Job logs error to stdout
2. Job updates database with error message
3. Job exits with error code
4. Cloud Run retries up to 3 times (configurable)
5. API notifies user of failure

Users can retry failed jobs from the UI.

## Contributing

### Adding New Features

1. Update blueprint schema if needed
2. Implement feature in job container
3. Add tests
4. Update documentation
5. Submit pull request

### Code Style

- Follow PEP 8 for Python code
- Use type hints
- Add docstrings to all functions
- Keep functions small and focused
- Write tests for all new code

### Testing

Run all tests before submitting:

```bash
# Unit tests
uv run python -m pytest job/test_*.py -v

# Integration tests
cd backend
uv run python test_blueprint_flow.py

# Performance tests
uv run python test_blueprint_performance.py
```

## License

See [LICENSE](../LICENSE) file for details.

## Support

For issues or questions:
1. Check this documentation
2. Review [Blueprint Schema](../docs/BLUEPRINT_SCHEMA.md)
3. Check [Troubleshooting](#troubleshooting) section
4. Review test files for examples
5. Open an issue on GitHub
